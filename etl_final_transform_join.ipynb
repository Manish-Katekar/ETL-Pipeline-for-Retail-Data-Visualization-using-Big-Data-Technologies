{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c441ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98d176ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "#os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "12cdc8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import col,when,create_map, lit,regexp_replace,isnull,trim\n",
    "from itertools import chain\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Create a SparkConf object to configure Spark properties\n",
    "conf = SparkConf()\n",
    "\n",
    "\n",
    "# Set the number of executors and driver memory requirements\n",
    "#conf.set(\"spark.executor.instances\", \"2\")  \n",
    "conf.set(\"spark.driver.memory\", \"1g\")  \n",
    "#conf.set(\"spark.executor.memory\", \"1g\")\n",
    "#conf.set(\"spark.executor.cores\", \"2\")\n",
    "conf.set(\"spark.sql.shuffle.partitions\", \"3\")\n",
    "conf.set(\"spark.sparkContext.defaultParallelism\", \"3\")\n",
    "# Create a SparkSession with the configured SparkConf\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark ETL\") \\\n",
    "    .master(\"local[3]\") \\\n",
    "    .config(\"spark.submit.deployMode\", \"client\") \\\n",
    "    .config(conf=conf) \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e6ef5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connection details\n",
    "url = \"jdbc:mysql://127.0.0.1:3306/test?useSSL=false&allowPublicKeyRetrieval=true\"\n",
    "driver = \"com.mysql.jdbc.Driver\"\n",
    "user = \"bigdata\"\n",
    "password = \"Bigdata@123\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ff8be15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.basicConfig(filename='error.log', level=logging.ERROR, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "def ingest_table(dbtable):\n",
    "        \"\"\"\n",
    "        Read data from table in database.\n",
    "\n",
    "        Args:\n",
    "            dbtable (DatabaseName_TableName): The input Tablename.\n",
    "        \n",
    "\n",
    "        Returns:\n",
    "            DataFrame: A DataFrame\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = spark.read \\\n",
    "                .format(\"jdbc\") \\\n",
    "                .option(\"driver\", driver) \\\n",
    "                .option(\"url\", url) \\\n",
    "                .option(\"user\", user) \\\n",
    "                .option(\"password\", password) \\\n",
    "                .option(\"dbtable\", dbtable) \\\n",
    "                .load()\n",
    "            return df.exceptAll(df.limit(1))\n",
    "        except Exception as e:\n",
    "            \n",
    "            logging.error(f\"Error ingesting table {dbtable}: {str(e)}\")\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        \n",
    "df_customers = ingest_table(\"project.customers\")\n",
    "df_payments = ingest_table(\"project.payments\")\n",
    "print(df_customers.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1846aae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "def ingest_csv(file_path):\n",
    "        \"\"\"\n",
    "        Read data from csv file in local file system.\n",
    "\n",
    "        Args:\n",
    "            file_path (Path of the file): The input filename.\n",
    "        \n",
    "\n",
    "        Returns:\n",
    "            Dataframe : A dataframe\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            df = spark.read \\\n",
    "                .format(\"csv\") \\\n",
    "                .option(\"header\", \"true\") \\\n",
    "                .option(\"inferSchema\", \"true\") \\\n",
    "                .load(file_path)\n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error ingesting CSV file {file_path}: {str(e)}\")\n",
    "            \n",
    "            return None\n",
    "        \n",
    "        \n",
    "df_orders = ingest_csv(\"file:///home/talentum/project/orders.csv\")\n",
    "df_order_items = ingest_csv(\"file:///home/talentum/project/order_items.csv\")\n",
    "df_products = ingest_csv(\"file:///home/talentum/project/products.csv\")\n",
    "print(df_orders.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0b2f89a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_products=df_products.drop(\"product_weight_g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a588b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_by_order_status(df, statuses_to_delete):\n",
    "    \"\"\"\n",
    "    Deletes rows from a DataFrame based on specified order statuses.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "        statuses_to_delete (list): A list of order statuses to be deleted.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with rows removed based on the order statuses.\n",
    "    \"\"\"\n",
    "    # Use the filter transformation to keep only rows with order statuses not in the specified list\n",
    "    cleaned_df = df.filter(~df[\"order_status\"].isin(statuses_to_delete))\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "df_orders=delete_rows_by_order_status(df_orders, [\"approved\",\"created\",\"invoiced\",\"processing\",\"shipped\",\"unavailable\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9720a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_orders = df_orders.filter(~((df_orders['order_status'] == 'delivered') & isnull(df_orders['order_delivered_timestamp'])))\n",
    "\n",
    "\n",
    "# Replace null values in the \"deliver_at\" column with a fixed date\n",
    "fixed_date = '01-01-2000 00:00'\n",
    "df_orders = df_orders.withColumn('order_delivered_timestamp', when(df_orders['order_delivered_timestamp'].isNull(), lit(fixed_date)).otherwise(df_orders['order_delivered_timestamp']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d8b553fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows_with_nulls(df_v):\n",
    "    \"\"\"\n",
    "    Removes rows from a DataFrame where any of the columns contain null (None) values.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The input DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A new DataFrame with rows containing null values removed.\n",
    "    \"\"\"\n",
    "    cleaned_df = df_v.na.drop()\n",
    "\n",
    "    return cleaned_df\n",
    "\n",
    "\n",
    "df_customers=remove_rows_with_nulls(df_customers)\n",
    "df_orders=remove_rows_with_nulls(df_orders)\n",
    "df_order_items=remove_rows_with_nulls(df_order_items)\n",
    "df_payments=remove_rows_with_nulls(df_payments)\n",
    "df_products=remove_rows_with_nulls(df_products)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5d1de48",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_customers = df_customers.withColumn(\"customer_state\", regexp_replace(col(\"customer_state\").cast(\"string\"), \"\\\\s+\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e82dfcce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "dictionary={'SP': 'São Paulo',\n",
    " 'SC': 'Santa Catarina',\n",
    " 'MG': 'Minas Gerais',\n",
    " 'PR': 'Paraná',\n",
    " 'RJ': 'Rio de Janeiro',\n",
    " 'RS': 'Rio Grande do Sul',\n",
    " 'PA': 'Pará',\n",
    " 'GO': 'Goiás',\n",
    " 'ES': 'Espírito Santo',\n",
    " 'BA': 'Bahia',\n",
    " 'MA': 'Maranhão',\n",
    " 'MS': 'Mato Grosso do Sul',\n",
    " 'CE': 'Ceará',\n",
    " 'DF': 'Distrito Federal',\n",
    " 'RN': 'Rio Grande do Norte',\n",
    " 'PE': 'Pernambuco',\n",
    " 'MT': 'Mato Grosso',\n",
    " 'AM': 'Amazonas',\n",
    " 'AP': 'Amapá',\n",
    " 'AL': 'Alagoas',\n",
    " 'RO': 'Rondônia',\n",
    " 'PB': 'Paraíba',\n",
    " 'TO': 'Tocantins',\n",
    " 'PI': 'Piauí',\n",
    " 'AC': 'Acre',\n",
    " 'SE': 'Sergipe',\n",
    " 'RR': 'Roraima'}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b2e8a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  \n",
    "mapping_expr = create_map([lit(x) for x in chain(*dictionary.items())])\n",
    "  \n",
    "        # Create a new column by calling the function to map the values\n",
    "        \n",
    "df_customers = df_customers.withColumn(\"state\",mapping_expr[col(\"customer_state\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "821ffd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_product={\n",
    "  \"perfumery\": \"Health and Beauty\",\n",
    "  \"art\": \"Art\",\n",
    "  \"sports_leisure\": \"Sports and Leisure\",\n",
    "  \"baby\": \"Baby\",\n",
    "  \"housewares\": \"Housewares\",\n",
    "  \"musical_instruments\": \"Musical Instruments\",\n",
    "  \"cool_stuff\": \"Cool Stuff\",\n",
    "  \"furniture_decor\": \"Furniture\",\n",
    "  \"home_appliances\": \"Home Appliances\",\n",
    "  \"toys\": \"Toys\",\n",
    "  \"bed_bath_table\": \"Homewares\",\n",
    "  \"construction_tools_safety\": \"Construction Tools\",\n",
    "  \"computers_accessories\": \"Electronics\",\n",
    "  \"health_beauty\": \"Health and Beauty\",\n",
    "  \"luggage_accessories\": \"Travel\",\n",
    "  \"garden_tools\": \"Garden\",\n",
    "  \"office_furniture\": \"Furniture\",\n",
    "  \"auto\": \"Automotive\",\n",
    "  \"electronics\": \"Electronics\",\n",
    "  \"fashion_shoes\": \"Fashion\",\n",
    "  \"telephony\": \"Telephony\",\n",
    "  \"stationery\": \"Stationery\",\n",
    "  \"fashion_bags_accessories\": \"Fashion\",\n",
    "  \"computers\": \"Electronics\",\n",
    "  \"home_construction\": \"Home Improvement\",\n",
    "  \"watches_gifts\": \"Gifts\",\n",
    "  \"construction_tools_construction\": \"Construction Tools\",\n",
    "  \"pet_shop\": \"Pets\",\n",
    "  \"small_appliances\": \"Home Appliances\",\n",
    "  \"agro_industry_and_commerce\": \"Business\",\n",
    "  \"NA\": \"Others\",\n",
    "  \"furniture_living_room\": \"Furniture\",\n",
    "  \"signaling_and_security\": \"Security\",\n",
    "  \"air_conditioning\": \"Home Appliances\",\n",
    "  \"consoles_games\": \"Entertainment\",\n",
    "  \"books_general_interest\": \"Books\",\n",
    "  \"costruction_tools_tools\": \"Construction Tools\",\n",
    "  \"fashion_underwear_beach\": \"Fashion\",\n",
    "  \"fashion_male_clothing\": \"Fashion\",\n",
    "  \"kitchen_dining_laundry_garden_furniture\": \"Furniture\",\n",
    "  \"industry_commerce_and_business\": \"Business\",\n",
    "  \"fixed_telephony\": \"Telephony\",\n",
    "  \"construction_tools_lights\": \"Construction Tools\",\n",
    "  \"books_technical\": \"Books\",\n",
    "  \"home_appliances_2\": \"Home Appliances\",\n",
    "  \"party_supplies\": \"Others\",\n",
    "  \"drinks\": \"Food and Drink\",\n",
    "  \"market_place\": \"Others\",\n",
    "  \"la_cuisine\": \"Home Appliances\",\n",
    "  \"costruction_tools_garden\": \"Garden\",\n",
    "  \"fashio_female_clothing\": \"Fashion\",\n",
    "  \"home_confort\": \"Home Improvement\",\n",
    "  \"audio\": \"Electronics\",\n",
    "  \"food_drink\": \"Food and Drink\",\n",
    "  \"music\": \"Entertainment\",\n",
    "  \"food\": \"Food and Drink\",\n",
    "  \"tablets_printing_image\": \"Electronics\",\n",
    "  \"books_imported\": \"Books\",\n",
    "  \"small_appliances_home_oven_and_coffee\": \"Home Appliances\",\n",
    "  \"fashion_sport\": \"Sports and Leisure\",\n",
    "  \"christmas_supplies\": \"Others\",\n",
    "  \"fashion_childrens_clothes\": \"Fashion\",\n",
    "  \"dvds_blu_ray\": \"Entertainment\",\n",
    "  \"arts_and_craftmanship\": \"Arts and Crafts\",\n",
    "  \"furniture_bedroom\": \"Furniture\",\n",
    "  \"cine_photo\": \"Entertainment\",\n",
    "  \"diapers_and_hygiene\": \"Baby\",\n",
    "  \"flowers\": \"Home and Garden\",\n",
    "  \"home_comfort_2\": \"Home Improvement\",\n",
    "  \"security_and_services\": \"Security\",\n",
    "  \"furniture_mattress_and_upholstery\": \"Furniture\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "38b19626",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_expr1 = create_map([lit(y) for y in chain(*dictionary_product.items())])\n",
    "\n",
    "df_products= df_products.withColumn(\"category\",mapping_expr1[col(\"product_category_name\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5b5a7173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.drop([\"order_approved_at\", \"order_delivered_timestamp\"])\n",
    "df_customers=df_customers.drop(\"customer_state\")\n",
    "df_products=df_products.drop(\"product_category_name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1aab93f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove leading and trailing spaces from the \"order_purchase_timestamp\" column\n",
    "df_orders = df_orders.withColumn(\"order_purchase_timestamp\", trim(col(\"order_purchase_timestamp\").cast(\"string\")))\n",
    "df_orders = df_orders.withColumn(\"order_approved_at\", trim(col(\"order_approved_at\").cast(\"string\")))\n",
    "df_orders = df_orders.withColumn(\"order_delivered_timestamp\", trim(col(\"order_delivered_timestamp\").cast(\"string\")))\n",
    "df_orders = df_orders.withColumn(\"order_estimated_delivery_date\", trim(col(\"order_estimated_delivery_date\").cast(\"string\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18b6c688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df_final = df_customers.alias(\"t1\").join(df_orders.alias(\"t2\"), col(\"t1.customer_id\") == col(\"t2.customer_id\")) \\\n",
    "    .join(df_payments.alias(\"t3\"), col(\"t2.order_id\") == col(\"t3.order_id\")) \\\n",
    "    .join(df_order_items.alias(\"t4\"), col(\"t3.order_id\") == col(\"t4.order_id\")) \\\n",
    "    .join(df_products.alias(\"t5\"), col(\"t4.product_id\") == col(\"t5.product_id\")) \\\n",
    "    .select(['t1.customer_id',\n",
    "             't4.order_id',\n",
    "             't1.customer_zip_code_prefix',\n",
    "             't1.customer_city',\n",
    "             't1.state',\n",
    "             't2.order_status',\n",
    "             't2.order_purchase_timestamp',\n",
    "             't2.order_approved_at',\n",
    "             't2.order_delivered_timestamp',\n",
    "             't2.order_estimated_delivery_date',\n",
    "             't3.payment_sequential',\n",
    "             't3.payment_type',\n",
    "             't3.payment_installments',\n",
    "             't3.payment_value',\n",
    "             't4.order_item_id',\n",
    "             't4.seller_id',\n",
    "             't4.price',\n",
    "             't4.shipping_charges',\n",
    "             't5.product_id',\n",
    "             't5.category',\n",
    "             't5.product_length_cm',\n",
    "             't5.product_height_cm',\n",
    "             't5.product_width_cm'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36c8b372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType\n",
    "\n",
    "# Define a custom schema for the desired columns\n",
    "custom_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_zip_code_prefix\", IntegerType(), True),\n",
    "    StructField(\"customer_city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"order_status\", StringType(), True),\n",
    "    StructField(\"order_purchase_timestamp\", TimestampType(), True),\n",
    "    StructField(\"order_approved_at\", TimestampType(), True),\n",
    "    StructField(\"order_delivered_timestamp\", TimestampType(), True),\n",
    "    StructField(\"order_estimated_delivery_date\", TimestampType(), True),\n",
    "    StructField(\"payment_sequential\", IntegerType(), True),\n",
    "    StructField(\"payment_type\", StringType(), True),\n",
    "    StructField(\"payment_installments\", IntegerType(), True),\n",
    "    StructField(\"payment_value\", DoubleType(), True),\n",
    "    StructField(\"order_item_id\", IntegerType(), True),\n",
    "    StructField(\"seller_id\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"shipping_charges\", DoubleType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"product_length_cm\", DoubleType(), True),\n",
    "    StructField(\"product_height_cm\", DoubleType(), True),\n",
    "    StructField(\"product_width_cm\", DoubleType(), True),\n",
    "])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d6a513c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_final.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"order_status\") \\\n",
    "    .option(\"path\", \"/home/talentum/retail_data\") \\\n",
    "    .saveAsTable(\"retail\",schema=custom_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6861157a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 25.575178861618042 seconds\n"
     ]
    }
   ],
   "source": [
    "# Your Spark code here\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Time taken: {elapsed_time} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
